## 主题 05：如何评估软件系统的吞吐量

### 1. 引言

在实践中经常会遇到需要进行系统性能优化的场景，通常系统性能优化的主要目标是提高系统的吞吐量，那么，系统的吞吐量与哪些因素有关呢？如何评估？是否有方法论可循？作为一名进阶路上的工程师，这三个问题你可曾思考过、总结过？如果在面试中遇到这样的提问，你是否能从容应对？

### 2. 单线程 RT

RT（Response Time）即响应时间，是评估软件系统性能的重要指标之一。RT 可以简单地理解为系统从输入到输出的时间间隔，系统可以指一个网站或者一个其它类型的软件应用，也可以指某个设备（比如手机，手机界面也有响应时间）。RT 本身是一个比较宽泛的概念，不过，在本文中 RT 特指互联网应用。对于服务端，RT 是指从服务器接收请求到响应该请求的全部数据被发往客户端。对于客户端，RT 是指从客户端（如浏览器）发起请求到客户端接收到响应该请求的全部数据并完全呈现的时间间隔，如下公式：

> 客户端的 RT = 服务器端的 RT + 网络传输时间 + 客户端呈现时间

其中，客户端呈现时间是指客户端（如浏览器）在接收到响应数据后呈现页面所需的时间，它与客户端处理器和软件应用性能有关。网络传输时间是指数据（包括请求数据和响应数据）在客户端和服务器端进行传输的时间，它与网络带宽有关，鉴于此，系统性能测试一般强调要在局域网中进行，因为局域网一般不会受到数据带宽的限制。

客户端的 RT 直接影响到用户体验，要降低客户端 RT 从而提升用户体验，通常必须考虑两点：一是服务器端的 RT，二是网络传输时间。为了降低网络传输时间，常见的优化方式有 CDN（Content Delivery Network）、ADN（Application Delivery Network）、专线，它们分别适用于不同的场景。

关于服务器端 RT，单线程场景下有如下公式：

> 服务器端 RT = Thread CPU Time + Thread Wait Time

从上面的公式可以看出，要降低 RT，就需要降低 Thread CPU Time 或者 Thread Wait Time。接下来，本文将将着重讨论服务器 RT、Thread CPU Time、Thread Wait Time 的相关知识。

### 3. 单线程 QPS

在上节中，我们简单地定义了服务器端 RT 的组成：Thread CPU Time + Thread Wait Time。如果系统只有一个线程，或者一个进程（该进程中只有一个线程），那么最大的 QPS（Queries Per Second）是多少呢？

> 单线程 QPS = 1000ms / RT（ RT 的单位也是 ms）

例如 RT 是 100ms（Thread CPU Time 为 20ms，Thread Wait Time 是 80ms），那么 1000ms 以内，系统可以接受的最大请求就是 1000ms/(20ms+80ms)=10。

### 4. 最佳线程数

继续采用上一节的例子（Thread CPU Time 为 20ms，Thread Wait Time 是 80ms），假设 CPU 的核心数是 1，只有一条线程，这个线程在处理某个请求的时候，CPU 真正用在这条线程上的时间就是 Thread CPU Time，也就是 20ms，那么在整个 RT 生命周期中，还有 80ms 的 Thread Wait Time，期间 CPU 在做什么呢？理想情况下，抛开系统层面的问题，可以简单地认为 CPU 在这 80ms 里什么也没做，至少对业务来说什么都没做。

**单核的情况**

由于每个请求过来，CPU 只需要工作 20ms 处理请求，因此在 80ms 的空闲时间里，我们可以认为系统还可以额外接受 80ms/20ms=4 个请求。考虑到同步，一个请求需要一条线程来处理，所以，我们就需要额外的 4 条线程来处理这些请求。如此，总的线程数其实就是 (80ms+21ms)/21ms=4.81 条，采用多线程后 Thread CPU Time 从 20ms 变成了 21ms，增加的 1ms 代表线程上下文切换、GC（如果是 JVM）等采用多线程模式带来的额外开销。当然，1ms 代表一个概数，这里只是为了便于读者理解。

**双核的情况**

在上面的例子中，一个核可以有 4.81 条线程，那么 2 个核呢，理想情况下，我们可以认为最佳线程数为：

> 2*(80ms+21ms)/21ms=9.62 条

**CPU 利用率**

上面的例子都是基于 CPU 在满负载下运行，在实际场景中是不太可能的。比如由于某个瓶颈，导致 CPU 得不到充分利用，总的 CPU 利用率低于 100%，假设为 N%，如此，服务器的最佳线程数为：

> N%*2*(80ms+21ms)/21ms

根据上面的分析，我们可以粗略得出单台服务器的最佳线程数公式：

> 单台服务器最佳线程数 = (RT/Thread CPU Time) * CPU cores * CPU利用率

上述最佳线程数公式并不是笔者胡编乱造的，在一些权威著作上都有论述，本文为了让读者更好地理解，采用循序渐进的方式引出这个公式。

### 5. 最大吞吐量

吞吐量是指单位时间内系统处理用户的请求数。从业务角度看，吞吐量可以用：请求数/秒、页面数/秒、人数/天或处理业务数/小时等单位来衡量；从网络角度看，吞吐量可以用：字节/秒来衡量。注意，吞吐量与并发数的区别，并发数用于衡量软件系统的并发处理能力，和吞吐量不同，它大多是占用套接字、句柄等操作系统资源。

如果已知一台服务器的最佳线程数和每条线程的 QPS，那么，线程数乘以每条线程的 QPS 即是这台机器在最佳线程数下的 QPS，也就是这台机器的最大吞吐量。基于此，可以得到下面的推算过程：

![在这里插入图片描述](https://images.gitbook.cn/4114d3b0-0819-11ea-9755-dbbfca9e4745)

约分处理：

![在这里插入图片描述](https://images.gitbook.cn/79b8f2f0-0819-11ea-bef3-6307f0c39f3e)

经过约分处理，可以得到单台服务器最大 QPS 的估算公式，如下：

![在这里插入图片描述](https://images.gitbook.cn/d0cde320-0819-11ea-9a91-b3dbf5ac73bd)

从上面的公式可以看出，决定单台服务器最大 QPS 的因素有 Thread CPU Time、CPU 核数以及 CPU 利用率。其中 CPU 核数是由硬件决定的，受限于成本，可优化空间有限，但是 Thread CPU Time 和 CPU 利用率却是与我们的代码紧密相关的。

需要说明的是，虽然从宏观上看上述公式是正确的，但其推导过程还是存在瑕疵的。由于多线程下的 Thread CPU Time 与单线程下的 Thread CPU Time 是不一样的，比如高并发下 GC 次数增加、线程上下文切换等都会消耗更多的 Thread CPU Time，所以按照上述公式推算出来的最大 QPS 和实际的最大 QPS 相比，会略微偏大。

关于线程切换开销，在同步模型下相同业务逻辑中单线程时的 Thread CPU Time 肯定会比多线程时的 Thread CPU Time 小，但如果采用的是异步模型，线程切换的开销会小很多，多线程和单线程的 Thread CPU Time 差距也就没那么大了，后文会详细描述。

基于上面的推论，决定单台服务器最大 QPS 的非硬件因素是 Thread CPU Time 和 CPU 利用率，那么这两个因子又是由什么决定的呢？

#### **5.1 Thread CPU Time**

Thread CPU Time 不只是业务逻辑所消耗的 CPU 时间，而是一次请求中所有环节上消耗的 CPU 时间之和。以 web 应用为例：一个请求中，HTTP 协议的解析所消耗的 CPU 时间，也属于 Thread CPU Time 的一部分。那么 Thread CPU Time 是由哪些因素决定的呢？两个关键——数据结构+算法，如果读者在 LeetCode 上刷过题，一定还记得提交答案后会出一个运行时间排名，其本质就是你的答案（代码）从开始执行到结束消耗的 CPU Time。比如下面这些问题，采用不同的数据结构和算法，消耗的 CPU Time 差距甚远。

- Top K 问题
- Hash 问题
- 排序和查找问题
- 序列化问题

#### **5.2 CPU 利用率**

在工程实践中，时常遇到 CPU 利用率偏低，从而影响系统可支持的最大 QPS 的问题。笔者大致梳理了一下可导致 CPU 利用率偏低的因素，如下所示：

- IO 能力（磁盘IO，网络 IO）；
- 网络带宽；
- 数据库连接池；
- 内存不足，比如 GC 大量占用 CPU，导致给业务逻辑的 CPU 利用率下降；
- 共享资源竞争，比如各种锁策略（读写锁、锁分离等）；
- 所依赖的其它后端服务吞吐量低造成瓶颈；
- 线程数或者进程数，编程模型（同步模型、异步模型，两者适用场景不同）；

上述影响 CPU 利用率的因素中，笔者遇到最多的是网络 IO 层面的问题，此外，GC 大量占用 CPU Time 也经常出现。

### 6. CPU 核心数与最大吞吐量

#### **6.1 Amdahl 定律**

Amdahl（阿姆达尔）定律是计算机系统设计的重要定量原理之一，于 1967 年由 IBM360 系列机的主要设计者 Amdahl 首先提出。该定律是指：系统中对某一部件采用更快执行方式所能获得的系统性能改进程度，取决于这种执行方式被使用的频率，或所占总执行时间的比例。阿姆达尔定律实际上定义了采取增强（加速）某部分功能处理的措施后可获得的性能改进或执行时间的加速比。

> 加速比 = 优化前系统耗时/优化后系统耗时

加速比越大，表明系统优化效果越明显。关于加速比的计算，在计算机领域还有另外一个公式，如下：

![在这里插入图片描述](https://images.gitbook.cn/894abd10-081a-11ea-9a91-b3dbf5ac73bd)

参数 F（系统内必须串行化的比重）相对而言比较难理解，在此，我举一个例子加以说明。如下所示，一个程序（计算机应用）包含 5 个步骤，每个步骤需耗时 100ms，其中步骤 2、步骤 4 可以并行，其它步骤只能串行。

![在这里插入图片描述](https://images.gitbook.cn/b9e20230-081a-11ea-9a91-b3dbf5ac73bd)

根据 Amdahl 定律，该系统的串行化的比重：

> F=3/5=0.6

由于该系统中，步骤 2、步骤 4 可以并行执行，因此，如果增加 CPU 的数量，以并行替代串行，便可以减少耗时，如下所示：

![在这里插入图片描述](https://images.gitbook.cn/4e9c0b50-081b-11ea-9755-dbbfca9e4745)

经过并行优化后：

> 加速比 = 500ms/400ms = 1.25

不难想见，如果 CPU 的数量无限多，那么步骤 2、步骤 4 的处理耗时将逼近 0，加速比的极限为：

> 500ms/300ms = 1.67

当然，加速比也可以通过上面的公式计算得出，如下所示：

![在这里插入图片描述](https://images.gitbook.cn/78468c50-081b-11ea-9a91-b3dbf5ac73bd)

#### **6.2 Gustafson 定律**

Gustafson（古斯塔夫森）定律是描述处理器个数、并行比例和加速比之间关系的另一个定律，与 Amdahl 定律齐名，被认为是 Amdahl 律的补充，公式如下所示，其中 N 是处理器个数，F 是串行时间的占总执行时间比例。

![在这里插入图片描述](https://images.gitbook.cn/e4a81a70-081c-11ea-9755-dbbfca9e4745)

从上述公式可以看出，如果一个系统中串行比例（参数 F）很小，那么加速比就是处理器的个数。比较 Gustafson 定律和 Amdahl 定律，很明显，两个定律计算出的最高加速比和最低加速比是一致的。基于上述两个定律，我们可以得出结论：对于一个系统，若无可并行的程序，加速比就是 1；若全部程序可并行，加速比就是 N。

#### **6.3 CPU 核心数和 Amdahl 定律的关系**

基于最大 QPS 的计算公式，在 Thread CPU Time 和 CPU 利用率不变的情况下，核心数越多，系统的最大 QPS 就能越大。举个例子，假设核心数从 1 增加到 4，同时 Thread CPU Time 和 CPU 利用率保持不变，则加速比为 4，最大 QPS 增加 4 倍。

上述例子，如果采用 Amdahl 定律来衡量，CPU 核心增加（即计算资源增加）4 倍，即参数 N 增大 4倍，而只要串行比 F 大于 0，最大 QPS 就不可能增大 4 倍。很明显，我们推导出来的最大 QPS 的计算公式与 Amdahl 定律计算出的加速比不一致，存在矛盾。

![在这里插入图片描述](https://images.gitbook.cn/894abd10-081a-11ea-9a91-b3dbf5ac73bd)

那么问题出在哪里呢？我们来回顾一下上述例子，其中有一个假设：CPU 核心数从 1 增加到 4 时，保持 Thread CPU Time 和 CPU 利用率不变。事实上，这个假设在实际情况下是不成立的，Thread CPU Time 和 CPU 利用率在大部分场景下是会变化的，因此，即便核心数增加 4 倍，最大 QPS 也增加不了 4 倍。

与此同时，Amdahl 定律中的 N 变化时，F 也可能会变化（上面已经举过例子），所以一般场景最大的 QPS 也增加不了 4 倍。至此，问题清楚了，两个公式并没有矛盾，相反它们在宏观上是一致的，相辅相成。需要注意的是，上文中笔者强调一般场景，如果是特殊场景——完全没有串行（程序没有串行，系统没有串行，无上下文切换，任何串行都没有），那么理论上还是可以增加 4 倍的。

#### **6.4 CPU 核数、利用率及系统串行部分比例之间的关系**

通过上一小节的介绍，我们知道，通常 CPU 核心数与服务器最大吞吐量之间的关系并非线性。那么，增加计算资源（本文指 CPU 核数）时，为什么最大 QPS 公式中的 CPU 利用率会变化？为什么 Amdahl 定律中的 F 也会变化？我们可以从宏观层面分析一下，当增加计算资源并达到满载时：

1. 系统 QPS 会增大，以 Java 应用为例，单位时间内产生的对象会增多，同等条件下，minor GC 被触发次数增加，甚至出现对象多到请求尚未返回，相关对象就进入 old 区的情况，从而 full GC 被触发的次数增加。对于最大 QPS 公式来说，CPU 利用率势必受到影响。
2. 同步模型下大量的线程在完成一次请求中，上下文被切换的次数大大增加。
3. 如果系统中存在串行模块，串行模块的执行和等待时间增加，Amdahl 定律中的 F 会变化，某些场景下 CPU 利用率也达不到理想效果。当然这取决于具体的代码实现，这就是为什么要做锁分离、要缩小同步块的原因。此外，锁自身的优化，如偏向、自旋、读写分离等技术，本质上都是为了减少 Amdahl 定律中的 F，减少 Thread CPU Time（锁本身的优化），提高 CPU 利用率（优化使用锁的方法）。

增加计算资源并承载更多的请求时，会引起最大 QPS 公式中两个参数的变化，也会引起 Amdahl 定律中 F 值的变化，同时公式和定律变化的趋势是相同的，在实践中，Amdahl 定律、最大 QPS 公式都有良好的指导意义。关于 CPU 核数、CPU 利用率以及系统中的串行部分比例，它们之间的关系可以用下图说明：

![在这里插入图片描述](https://images.gitbook.cn/62539f70-081e-11ea-9755-dbbfca9e4745)

从上面的关系图可以看出：

- 当计算资源（处理器核数）增加时，在串行部分比例不变的情况下，CPU 利用率下降；
- 当计算资源（处理器核数）增加时，且串行占的比例越大，CPU 利用率下降越明显。

### 7. 总结

本文主要介绍了软件系统最大吞吐量的估算公式，如下所示：

![在这里插入图片描述](https://images.gitbook.cn/d0cde320-0819-11ea-9a91-b3dbf5ac73bd)

此外，基于该公式可以延伸出几个知识点：

- 服务器最大 QPS 是由 Thread CPU Time、CPU 利用率、CPU 核数共同决定的；
- 服务器最佳线程数是由 Thread CPU Time、Thread Wait Time、CPU 核数及 CPU 利用率决定的；
- Thread CPU Time 是由数据结构和算法决定的；
- CPU 利用率与系统架构、串行部分比重、编程模型以及有无其它瓶颈相关；
- 性能优化必须考察 Thread CPU Time 降低的百分比和 CPU 利用率提高的百分比；
- 性能优化同时要考虑串行和并行的比例；
- 处理某个业务的最佳线程数存在一个临界点，超过临界点，最大 QPS 会下降，RT 也会明显增加。

> **引用与致谢：感谢阿里荣华老师对本文的贡献，特此感谢。**